sft_args:
  model_name_or_path: llm-jp/llm-jp-1.3b-v1.0
  tokenizer_name_or_path: llm-jp/llm-jp-1.3b-v1.0
  use_fast: True
  additional_special_tokens: null
  load_in_8bit: False
  load_in_4bit: True
  use_flash_attention_2: False
  max_seq_length: 2048
  data_files:
    - example/example.jsonl
  eval_data_files: null

lora:
  target_modules: all-linear
  r: 8
  lora_alpha: 32
  lora_dropout: 0.1

training_args:
  output_dir: output/rola_4bit
  overwrite_output_dir: False
  seed: 42

  do_train: True
  do_eval: True
  do_predict: False

  per_device_eval_batch_size: 8
  per_device_train_batch_size: 8

  optim: adamw_torch

  learning_rate: 2.0e-4
  warmup_ratio: 0.0
  warmup_steps: 0
  weight_decay: 0.0

  gradient_accumulation_steps: 1
  gradient_checkpointing: True
  gradient_checkpointing_kwargs: null

  logging_steps: 100
  logging_strategy: steps

  save_steps: 500
  save_strategy: steps
  save_total_limit: 1

  report_to: 'none'
